{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06f1a0e-6d4d-42e1-a8f3-56899e86dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3536cd33-e099-4572-9e50-5bf72c65989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca84311-c062-439c-b673-a0761fb7b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '..\\data\\ImageNet_2012'\n",
    "split = 'val'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256), # met la taille du plus petit côté de l'image à 256 (l'autre reste proportionnel par rapport au rapport initial)\n",
    "    transforms.CenterCrop(224), # sélectionne le carré de côté 224 à partir du centre de l'image\n",
    "    transforms.ToTensor(), # convertir image PIL en tenseur avec des valeurs comprises dans [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # normalise les valeurs du tenseur\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4308f589-ee0e-479f-9cd9-d727efbe2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation de la classe imagenet avec une transformation spécifique\n",
    "dataset = datasets.ImageNet(root, split=split, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a942d60-bea5-42da-a773-51d512de71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation du modèle\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0865e03-f214-47a5-9528-0ff8e5e08509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|████████████████████████████████████████████████████████████████████| 196/196 [01:59<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "dataloader = DataLoader(dataset, batch_size=256, num_workers=4, pin_memory=True) # dépend des specs de la machine\n",
    "correct_predictions = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(dataloader, desc=\"Evaluation\"):\n",
    "        # Copie des images/labels sur le GPU\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # On récupère les préictions du programme\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Obtention des classes prédites (pour chaque prédiction)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Ajout des prédictions correctes au total\n",
    "        correct_predictions += (predicted == labels).sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daaba77d-95f2-4a2d-a695-0df01a3b2e80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.342%\n"
     ]
    }
   ],
   "source": [
    "# Calcul du pourcentage de bonnes classifications\n",
    "accuracy = 100 * correct_predictions / len(dataset)\n",
    "\n",
    "print(f'Accuracy: {accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db80f134-6984-4879-b6cb-c89399c2d3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1, 2, 3], [4, 15, 6], [7, 8, 9]]).argmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf9eab8-4634-4b74-ab45-db49d0d54a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([2,4,2])/torch.tensor([2,2,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
