\documentclass[12pt,english, openany]{book}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Loading packages that alter the style
\usepackage[]{graphicx}
\usepackage[]{color}
\usepackage{alltt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{booktabs}
\usepackage{calc}
\usepackage{amsmath, amssymb}
\usepackage{bbold}
\usepackage[ruled,lined]{algorithm2e}
\usepackage{tikz}
\usepackage{cite}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\DeclareUnicodeCharacter{0301}{\'{ }}

% Package for figures
\usepackage{subfigure, float}

% Set page margins
\usepackage[top=100pt,bottom=100pt,left=68pt,right=66pt]{geometry}

% Package used for placeholder text
\usepackage{lipsum}

% Prevents LaTeX from filling out a page to the bottom
\raggedbottom

% Adding both languages
\usepackage[english, french]{babel}

% All page numbers positioned at the bottom of the page
\usepackage{fancyhdr}
\fancyhf{} % clear all header and footers
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0pt} % remove the header rule
\pagestyle{fancy}

% Changes the style of chapter headings
\usepackage{titlesec}
\titleformat{\chapter}
   {\normalfont\LARGE\bfseries}{\thechapter.}{1em}{}
% Change distance between chapter header and text
\titlespacing{\chapter}{0pt}{-10pt}{1\baselineskip}

% Adds table captions above the table per default
\usepackage{float}
\floatstyle{plaintop}
\restylefloat{table}

% Adds space between caption and table
\usepackage[tableposition=top]{caption}

% Adds hyperlinks to references and ToC
\usepackage{hyperref}
\hypersetup{hidelinks,linkcolor = black} % Changes the link color to black and hides the hideous red border that usually is created

% If multiple images are to be added, a folder (path) with all the images can be added here 
\graphicspath{ {images/} }

% Separates the first part of the report/thesis in Roman numerals
\frontmatter

\usepackage[nottoc, notlof, notlot]{tocbibind}

\bibliographystyle{plain}

%% Settings for the pseudocode
\SetKwFor{Tq}{tant que}{faire}{fin tq}%
\SetKwIF{Si}{SinonSi}{Sinon}{si}{alors}{sinon si}{sinon}{fin si}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Starts the document
\begin{document}

%%% Selects the language to be used for the first couple of pages
\selectlanguage{french}


%%%%% Adds the title page
\begin{titlepage}
	\clearpage\thispagestyle{empty}
	\centering
	\vspace{1cm}

	% Titles
	% Information about the University
	{\normalsize UTC \\ 
		PROJET TX 2024}
 \\
		
		\vspace{3cm}
	{\Huge \textbf{A guinea pig for deep learning with many classes}} \\

	\vspace{3cm}
	{\normalsize Deseure--Charron Alexis \\ Calzas Vincent \\ \par}
	\vspace{0.7cm}
 
    { \textbf{Référent école}} \\
    {Yves Grandvalet}
    \vspace{4.5cm}

    
    \centering 
    \includegraphics[scale=0.07]{images/UTC-SU.jpg}

	\pagebreak

\end{titlepage}

\tableofcontents{}

\newcommand{\hiddensubsection}[1]{
    \stepcounter{subsection}
    \subsection*{\hspace{1em}{#1}}
}

\mainmatter

\chapter{Introduction}

En Deep Learning, les problèmes de classifications peuvent composer plusieurs centaines voir milliers de classes. Balestriero a montré en 2022 \cite{balestriero2022effects} que les effets de la régularisation et de l'augmentation de données dépendent des classes du problème, ce qui signifie que la valeur optimale de l'hyper-paramètre de régularisation (exemple : le paramètre du weight decay) peut varier pour chacune des classes. L'hyper-paramètre est alors choisi en faisant un compromis entre les différentes classes en étant ainsi optimal pour aucune classe. \\

L'objectif de notre projet est alors d'étudier l'impact de l'ajustement de la dernière couche d'un modèle de classification en gelant les poids des couches précédentes déjà pré-entraînés. Pour cela, nous utiliserons un ResNet50 entraîné sur ImageNet afin de récupérer les sorties de l'avant-dernière couche et ainsi, à partir de ces résultats, ajuster l'entraînement d'un modèle de régression logistique correspondant à la dernière couche du modèle initial, pour étudier l'impact des régularisations.


\chapter{Aspects techniques}

\section{Notations}

Au cours de ce projet nous aurons souvent recours à l'utilisation de notations spécifiques, en voici les principales : 

\begin{itemize}
	\item $\mathcal{D}$, le jeu de données d'entrainement de $n \in \mathbb{N}$ éléments
	\item $c \in \mathbb{N}$, le nombre de classes du problème
    \item $p \in \mathbb{N}$, le nombre de neurones à l'avant dernière couche du modèle

\end{itemize}

\section{Génération des différents modèles au cours de l'entraînement}


\subsection{Sauvegarde les sorties de l'avant-dernière couche du modèle}

Dans un premier temps, étant donné que l'entraînement et la simple évaluation de millions d'images dans un réseau de neurones profond comme un ResNet50 est très coûteuse, le plus efficace est d'évaluer les sorties de taille $p$ obtenues à l'avant-dernière couche du réseau pour chacune des images de l'ensemble  $\mathcal{D}$ de données d'entraînement (dans notre cas, ImageNet), et de sauvegarder ces résultats dans des fichiers qui serviront par la suite à entraîner nos modèles de régression logistique.\\

En effet, étant donné que l'objectif est d'étudier l'évolution de notre fonction de perte pour chacune des classes au cours de l'apprentissage, il est nécessaire de sauvegarder nos modèles à différentes périodes/époques de l'apprentissage. De plus, au lieu de construire un modèle de régression logistique en gelant la descente de gradients sur les poids des couches précédant l'avant-dernière couche du modèle initial pendant l'entraînement, on obtiendrait les mêmes résultats mais l'évaluation, nécessaire des couches gelées, serait redondante et coûteuse inutilement.


\subsection{Entraînement du modèle de régression logistique}

À partir des sorties de l'avant-dernière couche du modèle initial (ResNet50), cela constitue un nouvel ensemble de données $\mathcal{O}$ composé d'autant de vecteurs qu'il y avait d'images initialement, c’est-à-dire $n$ vecteurs de taille $p$. En définissant ensuite un modèle de régression logistique composé d'une seule couche de sorties complètement connectée, on peut l'entraîner avec le dataset $\mathcal{O}$ et ainsi obtenir les valeurs de poids permettant de minimiser la fonction de perte et ainsi obtenir une précision optimale.\\

L'entraînement d'un tel modèle consiste à trouver la valeur des poids permettant d'avoir une fonction de coûts minimale. Dans notre cas, on utilisera l'entropie croisée $H$ définit pour $o$ la sortie réelle de taille $L$ attendue associée à une entrée quelconque et $\hat{o}$ la prédiction obtenue par le modèle :
$$
H(o, \hat{o}) = - \sum_{x = 1}^{L} o^{(x)} \log \hat{o}^{(x)}
$$
avec $o^{(x)}$ et $\hat{o}^{(x)}$ les valeurs d'indice $x$ des vecteurs $o$ et $\hat{o}$ respectivement.\\

Ensuite, afin de limiter le surapprentissage mais également dans le cadre de notre projet, nous rajoutons un weight decay à cette fonction de pertes afin d'obtenir la fonction complète qui sera minimisée : 
$$
L(o, \hat{o}) = H(o, \hat{o}) + \frac{\lambda }{2}\sum_l\sum_k (w^k_l)^2
$$
\section{Génération de différentes époques d'un modèle}

Ne disposant pas de modèle (très couteux à entrainer), nous avons décider de prendre le resnet50 qui est un modèle performant pour la classification. Pour avoir différentes époques de ce modèle, nous l'avons d'abord récupérer grâce à la deuxième méthode. En mettant en entrée la sortie du resnet50 à un modèle linéaire à une couche, nous simulons différentes époques du modèle resnet50. Pour finir, nous enregistrons les différentes époques du modèle (poids + biais) pour différents weight decay dans un fichier.

\section{Calcul des différents paramêtres de notre modèle}

On vient ensuite calculer la loss, le regret, le décalage et l'erreur par classe pour nos modèles (chaque weight decay).
Pour le calcul de la loss, on utilise la fonction cross-entropy plus le weight decay afin de minimiser le sur-apprentissage.

R(t) : consensus en fonction de l'époque t\\
Rk(t) consensus pour la classe k en fonction de l'époque t\\

t* = argmin(R(t))\\
tk = argmin(Rk(t)) \\
R* = min(R(t)) \\
Rk* = min(Rk(t))\\ 

Décalage : |t* - tk*|\\
Regret : |Rk - Rk(t*)|

Erreur par classe : fonction propre à pytorch

On vient ensuite enregistrer toutes ces données dans des fichiers.
\section{Traçage et visualisation des données}
--> mettre les courbes avec Figure ...

\section{Interprétation des données}
-> parler de l'accuracy qui a des paliers et qui montent de manière exponentiel\\
->la loss qui diminue et la comparée avec les autres weight decay => cohérent ?\\
-> Ques ce qu'on peut dire sur les biais en fonction des époques\\
-> Les différentes erreurs par classe qui n'atteignent pas en même temps leur minimum et qui remonte ensuite \\

=> But ralentir l'apprentissage 

\chapter{Ralentissement de l'apprentissage de certaines classes}


\chapter{Méthodologies}



	

 
% \begin{algorithm}[H]
% 	\caption{Titre}
%     \label{label}
% \end{algorithm}




\chapter{Résultats obtenus}


% \begin{figure}[H]
%   \subfigure[titre1]{\includegraphics[width=\linewidth, height=0.45\textheight]{images/plot1.png}}\hfill
%   \subfigure[titre2]{\includegraphics[width=\linewidth, height=0.45\textheight]{images/plot2.png}}\hfill
%   \caption{titre global}
%   \label{titre_global}
% \end{figure}


\chapter{Conclusion}


\bibliography{references.bib}


\end{document}
